{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOl3LhfoiGv1cDhd77tFy6V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarikaSatti06112001/Deep_Learning/blob/main/advanced_tensorflow_primitives_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD2QbEMDoP1d",
        "outputId": "346039d8-b2a0-44b7-b73b-52a14b21cc08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 4, 6])\n",
            "tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n",
            "[2 4 6]\n"
          ]
        }
      ],
      "source": [
        "#Scalar-Vector multiplication\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# PyTorch\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = 2\n",
        "result = y * x\n",
        "print(result)\n",
        "\n",
        "# TensorFlow\n",
        "x = tf.constant([1, 2, 3])\n",
        "y = 2\n",
        "result = tf.multiply(y, x)\n",
        "print(result)\n",
        "\n",
        "# NumPy\n",
        "x = np.array([1, 2, 3])\n",
        "y = 2\n",
        "result = y * x\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector-Vector multiplication"
      ],
      "metadata": {
        "id": "z02DfAa1osqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([4, 5, 6])\n",
        "result = x * y\n",
        "print(result)\n",
        "\n",
        "# TensorFlow\n",
        "x = tf.constant([1, 2, 3])\n",
        "y = tf.constant([4, 5, 6])\n",
        "result = tf.multiply(x, y)\n",
        "print(result)\n",
        "\n",
        "# NumPy\n",
        "x = np.array([1, 2, 3])\n",
        "y = np.array([4, 5, 6])\n",
        "result = x * y\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxYMOZi5ogLq",
        "outputId": "f1da2753-2af8-4e1b-f44d-cc3e50dd8457"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 4, 10, 18])\n",
            "tf.Tensor([ 4 10 18], shape=(3,), dtype=int32)\n",
            "[ 4 10 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outer product"
      ],
      "metadata": {
        "id": "jTH_MlRro3os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two 1D tensors\n",
        "a = tf.constant([1, 2, 3])\n",
        "b = tf.constant([4, 5, 6])\n",
        "\n",
        "# Reshape the tensors into 2D shape\n",
        "a_2d = tf.reshape(a, (3, 1))\n",
        "b_2d = tf.reshape(b, (1, 3))\n",
        "\n",
        "# Compute the outer product of a and b\n",
        "outer_prod = tf.matmul(a_2d, b_2d)\n",
        "\n",
        "print(outer_prod)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BguORXCao0ci",
        "outputId": "46e16f10-3302-4d5b-d01b-2f3f5df9a233"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 4  5  6]\n",
            " [ 8 10 12]\n",
            " [12 15 18]], shape=(3, 3), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scalar dot product"
      ],
      "metadata": {
        "id": "lYLgOXIzpwes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PyTorch\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([4, 5, 6])\n",
        "result = torch.dot(x, y)\n",
        "print(result)\n",
        "\n",
        "# TensorFlow\n",
        "x = tf.constant([1, 2, 3])\n",
        "y = tf.constant([4, 5, 6])\n",
        "result = tf.tensordot(x, y, axes=1)\n",
        "print(result)\n",
        "\n",
        "# NumPy\n",
        "x = np.array([1, 2, 3])\n",
        "y = np.array([4, 5, 6])\n",
        "result = np.dot(x, y)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9wX6bldpvzy",
        "outputId": "62b5f849-8658-48fd-e153-eb0a10d32171"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(32)\n",
            "tf.Tensor(32, shape=(), dtype=int32)\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hadamard product\n"
      ],
      "metadata": {
        "id": "YPhdltlcqbSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PyTorch\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([4, 5, 6])\n",
        "result = x * y\n",
        "print(result)\n",
        "\n",
        "# TensorFlow\n",
        "x = tf.constant([1, 2, 3])\n",
        "y = tf.constant([4, 5, 6])\n",
        "result = tf.multiply(x, y)\n",
        "print(result)\n",
        "\n",
        "# NumPy\n",
        "x = np.array([1, 2, 3])\n",
        "y = np.array([4, 5, 6])\n",
        "result = np.multiply(x, y)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpfrmOd-qXYG",
        "outputId": "105552b5-1d0a-42be-92d0-d055a5706d7c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 4, 10, 18])\n",
            "tf.Tensor([ 4 10 18], shape=(3,), dtype=int32)\n",
            "[ 4 10 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch matrix multiplication\n"
      ],
      "metadata": {
        "id": "v4lrrqUkrDW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PyTorch\n",
        "x = torch.rand\n"
      ],
      "metadata": {
        "id": "J3XltgEhq5pQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TENSOR CONTRACTION\n"
      ],
      "metadata": {
        "id": "iFeiN32RuO00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(2,3,5,7)\n",
        "b = torch.randn(11,13,3,17,5)\n",
        "torch.einsum('pqrs,tuqvr->pstuv', [a, b]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN7f9F2FuP0M",
        "outputId": "0f14a142-3505-4502-aaa7-f6102e71b4e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 7, 11, 13, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BILINEAR TRANSFORMATION\n",
        "a = torch.randn(2,3)\n",
        "b = torch.randn(5,3,7)\n",
        "c = torch.randn(2,7)\n",
        "torch.einsum('ik,jkl,il->ij', [a, b, c])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFJ_MszIuTa2",
        "outputId": "e2562f48-e022-4601-ce9b-0a26d39fb4d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0310, -5.4643, -5.8462,  5.4654, -2.5742],\n",
              "        [-1.4028,  0.3174, -2.9896,  3.5333,  6.1320]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def random_tensors(shape, num=1, requires_grad=False):\n",
        "  tensors = [torch.randn(shape, requires_grad=requires_grad) for i in range(0, num)]\n",
        "  return tensors[0] if num == 1 else tensors\n",
        "\n",
        "# Parameters\n",
        "# -- [num_actions x hidden_dimension]\n",
        "b = random_tensors([5, 3], requires_grad=True)\n",
        "# -- [num_actions x hidden_dimension x hidden_dimension]\n",
        "W = random_tensors([5, 3, 3], requires_grad=True)\n",
        "\n",
        "def transition(zl):\n",
        "  # -- [batch_size x num_actions x hidden_dimension]\n",
        "  return zl.unsqueeze(1) + F.tanh(torch.einsum(\"bk,aki->bai\", [zl, W]) + b)\n",
        "\n",
        "# Sampled dummy inputs\n",
        "# -- [batch_size x hidden_dimension]\n",
        "zl = random_tensors([2, 3])\n",
        "\n",
        "transition(zl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4OKg3PeubhW",
        "outputId": "41c64472-7a1e-401c-b80f-d0f45ee4db3e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-2.0845,  0.1996,  1.3517],\n",
              "         [-1.5947,  0.6471,  1.4966],\n",
              "         [-0.0954,  1.1319,  0.1072],\n",
              "         [-0.6178,  0.4256,  1.2624],\n",
              "         [-2.0508, -0.0833,  1.3276]],\n",
              "\n",
              "        [[-0.2545,  1.2938, -0.9347],\n",
              "         [-0.9028,  2.6235,  0.3607],\n",
              "         [ 0.6876,  2.5235,  0.8626],\n",
              "         [ 0.7556,  0.8940,  0.8621],\n",
              "         [-1.2245,  0.6520,  0.8421]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ATTENTION"
      ],
      "metadata": {
        "id": "wzEiGPwNul_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bM, br, w = random_tensors([7], num=3, requires_grad=True)\n",
        "# -- [hidden_dimension x hidden_dimension]\n",
        "WY, Wh, Wr, Wt = random_tensors([7, 7], num=4, requires_grad=True)\n",
        "\n",
        "# Single application of attention mechanism \n",
        "def attention(Y, ht, rt1):\n",
        "  # -- [batch_size x hidden_dimension] \n",
        "  tmp = torch.einsum(\"ik,kl->il\", [ht, Wh]) + torch.einsum(\"ik,kl->il\", [rt1, Wr])\n",
        "  Mt = F.tanh(torch.einsum(\"ijk,kl->ijl\", [Y, WY]) + tmp.unsqueeze(1).expand_as(Y) + bM)\n",
        "  # -- [batch_size x sequence_length]\n",
        "  at = F.softmax(torch.einsum(\"ijk,k->ij\", [Mt, w])) \n",
        "  # -- [batch_size x hidden_dimension]\n",
        "  rt = torch.einsum(\"ijk,ij->ik\", [Y, at]) + F.tanh(torch.einsum(\"ij,jk->ik\", [rt1, Wt]) + br)\n",
        "  # -- [batch_size x hidden_dimension], [batch_size x sequence_dimension]\n",
        "  return rt, at\n",
        "\n",
        "# Sampled dummy inputs\n",
        "# -- [batch_size x sequence_length x hidden_dimension]\n",
        "Y = random_tensors([3, 5, 7])\n",
        "# -- [batch_size x hidden_dimension]\n",
        "ht, rt1 = random_tensors([3, 7], num=2)\n",
        "\n",
        "rt, at = attention(Y, ht, rt1)\n",
        "at  # -- print attention weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mSgWz4Buhuu",
        "outputId": "a6e71d91-5dec-458d-ac14-59e80218a8d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-f7e5857da595>:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  at = F.softmax(torch.einsum(\"ijk,k->ij\", [Mt, w]))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2195, 0.0891, 0.3087, 0.2711, 0.1116],\n",
              "        [0.1431, 0.2479, 0.1838, 0.2142, 0.2110],\n",
              "        [0.1140, 0.4318, 0.1593, 0.1182, 0.1768]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}